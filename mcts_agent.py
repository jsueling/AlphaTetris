"""MCTS + DRL agent that learns how to play Tetris"""

import os
import time
import multiprocessing as mp
import random

import numpy as np
import torch
from tqdm import tqdm

from mcts import MonteCarloTreeNode
from experience_replay_buffer import ExperienceReplayBuffer
from tetris_env import Tetris
from model import A0ResNet, ResNet18
from score_normaliser import ScoreNormaliser
from inference_server import InferenceServer

MCTS_ITERATIONS = 1600 # Number of MCTS iterations per action selection
ACTION_SPACE = 40 # Upper bound on possible actions for hard drop (rotations * columns placements)
BATCH_SIZE = 256 # Batch size for experience replay
RANDOM_SEED_MODULUS = 2 ** 32 # Seed methods accept 32-bit integers only

class MCTSAgent:
    """MCTS + DRL agent for playing Tetris."""
    def __init__(self, batch_size=BATCH_SIZE):
        # self.model = A0ResNet(num_residual_blocks=19, num_actions=ACTION_SPACE)
        self.model = ResNet18([2, 2, 2, 2], num_actions=ACTION_SPACE)
        self.buffer = ExperienceReplayBuffer(
            batch_size=batch_size,
            max_size=1000000,
            device=self.model.device
        )
        self.batch_size = batch_size
        self.env = Tetris()
        self.score_normaliser = ScoreNormaliser()

    def update(self):
        """Update the agent's model via experience replay"""

        states, tree_policies, rewards_to_go, legal_actions_masks = self.buffer.sample()

        self.model.train()
        self.model.optimiser.zero_grad()
        loss = self.model.loss(
            states,
            tree_policies,
            rewards_to_go,
            legal_actions_masks
        )
        loss.backward()
        self.model.optimiser.step()

    def train_ensemble(self, episodes=10000, n_workers=10):
        """
        Run the training loop for an MCTS agent that uses ensemble determinisation
        to play Tetris. Each step in an episode consists of running parallel
        determinised MCTS trajectories and taking the majority vote on action
        selection using the average visit counts of the ensemble. The agent learns
        from the transitions generated by the MCTS ensemble.
        """

        os.makedirs("./out", exist_ok=True)

        prev_step_count = step_count = 0
        episode_scores = []
        rolling_avg_scores = []
        steps_per_episode = []
        episode_times = []

        # Pre-allocate queues for parallel MCTS simulations. Each worker has its own response queue
        # to avoid contention when receiving responses from the queue.
        response_queues = { worker_id: mp.Queue() for worker_id in range(n_workers) }
        request_queue = mp.Queue()
        result_queue = mp.Queue()

        # Create and start the inference server which handles model inference
        # requests sent by the MCTS workers.
        inference_server = InferenceServer(
            model=self.model,
            request_queue=request_queue,
            response_queues=response_queues
        )
        inference_server.start()

        for _ in tqdm(range(episodes)):

            start_time = time.time()

            # Reset the environment per episode, generating the first Tetromino of the sequence
            self.env.reset()
            transitions = []
            done = False
            while not done:

                # Since Tetromino generation is stochastic, the old tree must be discarded
                # after each step in the actual environment.

                # Take the majority vote of the MCTS ensemble on which action to take and
                # the optimal tree policy for the given node.
                chosen_action, available_actions, tree_policy =  run_ensemble_mcts(
                    self.env,
                    request_queue,
                    response_queues,
                    result_queue,
                    step_count,
                    prev_step_count,
                    n_workers=n_workers
                )

                score_before_action = self.env.score
                state_before_action = self.env.get_state()

                legal_actions = np.zeros(ACTION_SPACE, dtype=np.float32)
                # Upon root node expansion, child nodes are created for each legal action
                legal_actions[available_actions] = 1.0

                transitions.append([
                    state_before_action,
                    tree_policy,
                    legal_actions,
                    score_before_action
                ])

                if all([
                    # Allows for sufficient diversity in transitions before sampling
                    step_count >= 1e3,
                    # 1:1 ratio of data generated-to-consumed means that
                    # each transition is expected to be sampled for training
                    # once before being replaced on average.
                    step_count % self.batch_size == 0
                ]):
                    self.update()

                # Update the environment with the selected action and next Tetromino
                done, _ = self.env.step(chosen_action)
                self.env.create_tetromino(self.env.generate_next_tetromino_type())

                step_count += 1

            # Episode end

            final_score = self.env.score

            episode_scores.append(final_score)
            rolling_avg_scores.append(np.mean(episode_scores[-100:]))
            steps_per_episode.append(step_count - prev_step_count)
            end_time = time.time()
            episode_times.append(end_time - start_time)

            results = {
                "episode_scores": episode_scores,
                "rolling_avg_scores": rolling_avg_scores,
                "steps_per_episode": steps_per_episode,
                "episode_times": episode_times,
                "total_steps": step_count,
                "total_episodes": len(episode_scores),
            }

            np.save("./out/tetris_mcts_drl_results.npy", np.array(results))

            prev_step_count = step_count

            # After each episode, calculate the return-to-go (RTG) and normalise
            # it to range [-1, 1] based on rolling average agent score since Tetris
            # scores are unbounded. This should encourage the agent to learn
            # to play moves (visit actions) that score better than its current iteration
            # when used in MCTS node selection in deciding which children to visit - Q(s,a).
            # The assumption is that moves that were improvements on average score in
            # the past should also be improvements on average score in the future which is a
            # reasonable assumption for Tetris due to the repeated nature of the game.

            states = np.stack([t[0] for t in transitions])
            tree_policies = np.stack([t[1] for t in transitions])
            legal_actions_masks = np.stack([t[2] for t in transitions])
            scores_before_action = np.stack([t[3] for t in transitions])
            rewards_to_go = final_score - scores_before_action
            # Calculate normalised rewards-to-go (RTG)
            normalised_rewards_to_go = self.score_normaliser.normalise(rewards_to_go)
            # Add all transitions to the experience replay buffer
            self.buffer.add_transitions_batch(
                states,
                tree_policies,
                normalised_rewards_to_go,
                legal_actions_masks
            )
            # Update the normalising factor per batch of rewards-to-go
            self.score_normaliser.update(rewards_to_go)

        inference_server.stop()

def run_ensemble_mcts(
        env: Tetris,
        request_queue: mp.Queue,
        response_queues: dict[int, mp.Queue],
        result_queue: mp.Queue,
        step_count,
        prev_step_count,
        n_workers,
        exploratory_step_threshold=30,
        tau=1.0 # Temperature parameter for action selection
    ):
    """
    Multiple workers run MCTS iterations in parallel from the root node.
    The stochastic outcome (Tetromino generation) is determinised per
    node expansion i.e. a single future is sampled and shared among child nodes.
    Therefore, we take the average/majority vote from many parallel determinisations
    to get a more robust action decision/transitions at the root.

    Returns:
    - chosen_action: The action selected by the MCTS ensemble
    - available_actions: Actions available to the root node
    - tree_policy: The probability distribution over actions given by mean visit counts
    """

    processes = []

    for worker_id in range(n_workers):

        # Each process has its own seed for reproducibility and to prevent the same
        # random number generation (unique in base n_workers)
        process_seed = (step_count * n_workers + worker_id) % RANDOM_SEED_MODULUS

        p = mp.Process(
            target=ensemble_mcts_helper,
            args=(
                env.copy(),
                request_queue,
                response_queues[worker_id],
                worker_id,
                result_queue,
                MCTS_ITERATIONS,
                process_seed
            )
        )
        p.start()
        processes.append(p)

    # Block until all processes are finished
    for p in processes:
        p.join()

    # Fetch results from all processes
    root_nodes = [result_queue.get() for _ in processes]

    # Stack visit counts from all root nodes and compute the mean
    mean_visit_counts = np.mean(
        [node["visit_counts"] for node in root_nodes],
        axis=0
    )

    # The next action is decided based on the visit counts of the actions
    # available to the root node in the simulations. The tree policy is the
    # probability distribution over those actions

    tree_policy = np.zeros(ACTION_SPACE, dtype=np.float32)

    # All actions available to the root node (same for any of the root nodes)
    available_actions = root_nodes[0]["actions"]

    steps_since_episode_start = step_count - prev_step_count

    # If no actions are available, the game will end
    if len(available_actions) == 0:
        chosen_action = -1
    # For the first 30 steps of each episode, allow exploration where
    # tau modulates exploration in action selection.
    elif steps_since_episode_start < exploratory_step_threshold:
        mean_visit_counts = mean_visit_counts ** (1 / tau)
        visit_probabilities = mean_visit_counts / np.sum(mean_visit_counts)
        tree_policy[available_actions] = visit_probabilities
        chosen_action = np.random.choice(ACTION_SPACE, p=tree_policy)
    # Otherwise, greedily select the action with the highest visit count
    else:
        chosen_action = available_actions[np.argmax(mean_visit_counts)]
        tree_policy[chosen_action] = 1.0

    return chosen_action, available_actions, tree_policy

def ensemble_mcts_helper(
        env: Tetris,
        request_queue: mp.Queue,
        response_queue: mp.Queue,
        worker_id: int,
        result_queue: mp.Queue,
        iterations: int,
        process_seed: int,
    ):
    """
    Helper function used by a single process for parallel simulations that runs MCTS
    iterations on a single node and puts the results into the result queue.
    """

    np.random.seed(process_seed)
    torch.manual_seed(process_seed)
    random.seed(process_seed)

    root_node = MonteCarloTreeNode(
        env=env,
        request_queue=request_queue,
        response_queue=response_queue,
        worker_id=worker_id,
        is_root=True
    )

    for _ in range(iterations):
        root_node.run_iteration()

    result_queue.put({
        "actions": list(root_node.children.keys()),
        "visit_counts": [child.visit_count for child in root_node.children.values()]
    })
